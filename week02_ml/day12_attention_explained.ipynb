{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:05:40.395178900Z",
     "start_time": "2026-02-04T15:04:05.333589300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# ЗАДАЧА 1: Визуализация Attention\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Загружаем модель с включенным выводом attention весов\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_attentions=True)\n",
    "model.eval()  # переводим в режим оценки\n",
    "\n",
    "# Токенизируем предложение\n",
    "text = \"The cat sat on the mat\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Прямой проход через модель\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# В outputs.attentions лежит список тензоров для каждого из 12 слоев\n",
    "# Формат: (batch_size, num_heads, seq_len, seq_len)\n",
    "attentions = outputs.attentions\n",
    "\n",
    "print(f\"Количество слоев: {len(attentions)}\")\n",
    "print(f\"Форма attention одного слоя: {attentions[0].shape}\")\n",
    "print(f\"Количество attention heads: {attentions[0].shape[1]}\")\n",
    "\n",
    "# Посмотрим, как выглядит токенизация\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(\"\\nТокены:\", tokens)\n",
    "print(\"ID токенов:\", inputs['input_ids'][0].tolist())\n",
    "\n",
    "\n",
    "def visualize_attention(layer_idx=0, head_idx=0, word=\"cat\"):\n",
    "    \"\"\"Визуализация attention для конкретного слова\"\"\"\n",
    "\n",
    "    # Находим индекс токена для нужного слова\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    word_index = tokenized_text.index(word) + 1  # +1 из-за [CLS]\n",
    "\n",
    "    # Берем attention веса из указанного слоя и head\n",
    "    attention = attentions[layer_idx][0, head_idx].numpy()\n",
    "\n",
    "    # Визуализируем\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attention, cmap='Reds', vmin=0, vmax=1)\n",
    "\n",
    "    # Настройки графика\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=45)\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.title(f\"Layer {layer_idx}, Head {head_idx}, Word: '{word}'\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Покажем численно, на какие токены смотрит \"cat\"\n",
    "    print(f\"\\nAttention от слова '{word}' (индекс {word_index}):\")\n",
    "    for i, (token, weight) in enumerate(zip(tokens, attention[word_index])):\n",
    "        print(f\"{token:10s}: {weight:.4f}\")\n",
    "\n",
    "\n",
    "# Смотрим внимания для разных конфигураций\n",
    "visualize_attention(layer_idx=0, head_idx=0, word=\"cat\")\n",
    "visualize_attention(layer_idx=5, head_idx=3, word=\"sat\")\n",
    "\n",
    "\n",
    "def analyze_word_attention(word=\"the\"):\n",
    "    \"\"\"Анализируем, как меняется внимание к слову по слоям\"\"\"\n",
    "    word_idx = tokens.index(word) if word in tokens else -1\n",
    "\n",
    "    if word_idx == -1:\n",
    "        print(f\"Слово '{word}' не найдено в токенах\")\n",
    "        return\n",
    "\n",
    "    all_attentions = []\n",
    "\n",
    "    # Собираем attention веса по всем слоям и head'ам\n",
    "    for layer_idx in range(len(attentions)):\n",
    "        layer_attention = attentions[layer_idx][0, :, word_idx, :].mean(0)\n",
    "        all_attentions.append(layer_attention.numpy())\n",
    "\n",
    "    # Визуализируем тепловую карту по слоям\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(np.array(all_attentions), aspect='auto', cmap='Reds')\n",
    "    plt.xlabel('Токены')\n",
    "    plt.ylabel('Слои')\n",
    "    plt.yticks(range(len(attentions)))\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=45)\n",
    "    plt.title(f\"Эволюция внимания к слову '{word}' по слоям BERT\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "analyze_word_attention(\"cat\")"
   ],
   "id": "6f3db97be384b461",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "815689697a4849628043900a99c70784"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Загружаем модель с включенным выводом attention весов\u001B[39;00m\n\u001B[32m      7\u001B[39m tokenizer = BertTokenizer.from_pretrained(\u001B[33m'\u001B[39m\u001B[33mbert-base-uncased\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m model = \u001B[43mBertModel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbert-base-uncased\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m                                  \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m model.eval()  \u001B[38;5;66;03m# переводим в режим оценки\u001B[39;00m\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Токенизируем предложение\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:4038\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   4033\u001B[39m     logger.warning_once(\n\u001B[32m   4034\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mA kernel_config was provided but use_kernels is False; setting use_kernels=True automatically. To suppress this warning, explicitly set use_kernels to True.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4035\u001B[39m     )\n\u001B[32m   4036\u001B[39m     use_kernels = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4038\u001B[39m checkpoint_files, sharded_metadata = \u001B[43m_get_resolved_checkpoint_files\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4039\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4040\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvariant\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvariant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4041\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgguf_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgguf_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4042\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_safetensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_safetensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4043\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdownload_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdownload_kwargs_with_commit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4044\u001B[39m \u001B[43m    \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4045\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_auto_class\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   4046\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtransformers_explicit_filename\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtransformers_weights\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4047\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4049\u001B[39m is_quantized = hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   4051\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m gguf_file:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_utils.py:641\u001B[39m, in \u001B[36m_get_resolved_checkpoint_files\u001B[39m\u001B[34m(pretrained_model_name_or_path, variant, gguf_file, use_safetensors, user_agent, is_remote_code, transformers_explicit_filename, download_kwargs)\u001B[39m\n\u001B[32m    629\u001B[39m can_auto_convert = (\n\u001B[32m    630\u001B[39m     \u001B[38;5;129;01mnot\u001B[39;00m is_offline_mode()  \u001B[38;5;66;03m# for obvious reasons\u001B[39;00m\n\u001B[32m    631\u001B[39m     \u001B[38;5;66;03m# If we are in a CI environment or in a pytest run, we prevent the conversion\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    634\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m subfolder == \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m  \u001B[38;5;66;03m# converter bot does not work on subfolders\u001B[39;00m\n\u001B[32m    635\u001B[39m )\n\u001B[32m    637\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    638\u001B[39m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[32m    639\u001B[39m     \u001B[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001B[39;00m\n\u001B[32m    640\u001B[39m     \u001B[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m641\u001B[39m     resolved_archive_file = \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcached_file_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    643\u001B[39m     \u001B[38;5;66;03m# Try safetensors files first if not already found\u001B[39;00m\n\u001B[32m    644\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m resolved_archive_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001B[32m    645\u001B[39m         \u001B[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\hub.py:276\u001B[39m, in \u001B[36mcached_file\u001B[39m\u001B[34m(path_or_repo_id, filename, **kwargs)\u001B[39m\n\u001B[32m    221\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcached_file\u001B[39m(\n\u001B[32m    222\u001B[39m     path_or_repo_id: \u001B[38;5;28mstr\u001B[39m | os.PathLike,\n\u001B[32m    223\u001B[39m     filename: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m    224\u001B[39m     **kwargs,\n\u001B[32m    225\u001B[39m ) -> \u001B[38;5;28mstr\u001B[39m | \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    226\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    227\u001B[39m \u001B[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001B[39;00m\n\u001B[32m    228\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    274\u001B[39m \u001B[33;03m    ```\u001B[39;00m\n\u001B[32m    275\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m276\u001B[39m     file = \u001B[43mcached_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilenames\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    277\u001B[39m     file = file[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m file\n\u001B[32m    278\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m file\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\hub.py:419\u001B[39m, in \u001B[36mcached_files\u001B[39m\u001B[34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[39m\n\u001B[32m    416\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    417\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(full_filenames) == \u001B[32m1\u001B[39m:\n\u001B[32m    418\u001B[39m         \u001B[38;5;66;03m# This is slightly better for only 1 file\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m419\u001B[39m         \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    420\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    421\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfilenames\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    422\u001B[39m \u001B[43m            \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    423\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    424\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    425\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    426\u001B[39m \u001B[43m            \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    427\u001B[39m \u001B[43m            \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    428\u001B[39m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    429\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    430\u001B[39m \u001B[43m            \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    431\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    432\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    433\u001B[39m         snapshot_download(\n\u001B[32m    434\u001B[39m             path_or_repo_id,\n\u001B[32m    435\u001B[39m             allow_patterns=full_filenames,\n\u001B[32m   (...)\u001B[39m\u001B[32m    443\u001B[39m             local_files_only=local_files_only,\n\u001B[32m    444\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     85\u001B[39m         validate_repo_id(arg_value)\n\u001B[32m     87\u001B[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001B[34m__name__\u001B[39m, kwargs=kwargs)\n\u001B[32m---> \u001B[39m\u001B[32m89\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:1024\u001B[39m, in \u001B[36mhf_hub_download\u001B[39m\u001B[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, tqdm_class, dry_run)\u001B[39m\n\u001B[32m   1003\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_local_dir(\n\u001B[32m   1004\u001B[39m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[32m   1005\u001B[39m         local_dir=local_dir,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1021\u001B[39m         dry_run=dry_run,\n\u001B[32m   1022\u001B[39m     )\n\u001B[32m   1023\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1025\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[32m   1026\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1027\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[32m   1028\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1029\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1030\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1031\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1032\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[32m   1033\u001B[39m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1034\u001B[39m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1035\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1036\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1037\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[32m   1038\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1039\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1040\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtqdm_class\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtqdm_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1041\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdry_run\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdry_run\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1042\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:1240\u001B[39m, in \u001B[36m_hf_hub_download_to_cache_dir\u001B[39m\u001B[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, tqdm_class, dry_run)\u001B[39m\n\u001B[32m   1237\u001B[39m \u001B[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001B[39;00m\n\u001B[32m   1239\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m WeakFileLock(lock_path):\n\u001B[32m-> \u001B[39m\u001B[32m1240\u001B[39m     \u001B[43m_download_to_tmp_and_move\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1241\u001B[39m \u001B[43m        \u001B[49m\u001B[43mincomplete_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblob_path\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m.incomplete\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1242\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdestination_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblob_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1243\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1244\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1245\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1246\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1247\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1248\u001B[39m \u001B[43m        \u001B[49m\u001B[43metag\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mxet_file_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mxet_file_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtqdm_class\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtqdm_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1251\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1252\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.exists(pointer_path):\n\u001B[32m   1253\u001B[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:1864\u001B[39m, in \u001B[36m_download_to_tmp_and_move\u001B[39m\u001B[34m(incomplete_path, destination_path, url_to_download, headers, expected_size, filename, force_download, etag, xet_file_data, tqdm_class)\u001B[39m\n\u001B[32m   1862\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m xet_file_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m is_xet_available():\n\u001B[32m   1863\u001B[39m     logger.debug(\u001B[33m\"\u001B[39m\u001B[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1864\u001B[39m     \u001B[43mxet_get\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1865\u001B[39m \u001B[43m        \u001B[49m\u001B[43mincomplete_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mincomplete_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1866\u001B[39m \u001B[43m        \u001B[49m\u001B[43mxet_file_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mxet_file_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1867\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1868\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1869\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisplayed_filename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1870\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtqdm_class\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtqdm_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1871\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1872\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1873\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m xet_file_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m constants.HF_HUB_DISABLE_XET:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:588\u001B[39m, in \u001B[36mxet_get\u001B[39m\u001B[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, tqdm_class, _tqdm_bar)\u001B[39m\n\u001B[32m    585\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprogress_updater\u001B[39m(progress_bytes: \u001B[38;5;28mfloat\u001B[39m):\n\u001B[32m    586\u001B[39m     progress.update(progress_bytes)\n\u001B[32m--> \u001B[39m\u001B[32m588\u001B[39m \u001B[43mdownload_files\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    589\u001B[39m \u001B[43m    \u001B[49m\u001B[43mxet_download_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    590\u001B[39m \u001B[43m    \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconnection_info\u001B[49m\u001B[43m.\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    591\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_info\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconnection_info\u001B[49m\u001B[43m.\u001B[49m\u001B[43maccess_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconnection_info\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexpiration_unix_epoch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    592\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_refresher\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_refresher\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    593\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprogress_updater\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mprogress_updater\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    594\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def simple_self_attention(X, d_k):\n",
    "    \"\"\"\n",
    "    Простая реализация self-attention без обучения весов\n",
    "\n",
    "    X: входные эмбеддинги [seq_len, d_model]\n",
    "    d_k: размерность ключей/запросов\n",
    "    \"\"\"\n",
    "    seq_len, d_model = X.shape\n",
    "\n",
    "    # Инициализируем веса случайно (в реальной модели они обучаются)\n",
    "    np.random.seed(42)\n",
    "    W_q = np.random.randn(d_model, d_k) * 0.1\n",
    "    W_k = np.random.randn(d_model, d_k) * 0.1\n",
    "    W_v = np.random.randn(d_model, d_model) * 0.1\n",
    "\n",
    "    # Шаг 1: Получаем Q, K, V\n",
    "    Q = X @ W_q  # [seq_len, d_k]\n",
    "    K = X @ W_k  # [seq_len, d_k]\n",
    "    V = X @ W_v  # [seq_len, d_model]\n",
    "\n",
    "    print(f\"Q shape: {Q.shape}\")\n",
    "    print(f\"K shape: {K.shape}\")\n",
    "    print(f\"V shape: {V.shape}\")\n",
    "\n",
    "    # Шаг 2: Вычисляем attention scores\n",
    "    attention_scores = Q @ K.T  # [seq_len, seq_len]\n",
    "    print(f\"\\nAttention scores shape: {attention_scores.shape}\")\n",
    "\n",
    "    # Шаг 3: Масштабируем и применяем softmax\n",
    "    attention_scores = attention_scores / np.sqrt(d_k)\n",
    "    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n",
    "\n",
    "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "    # Шаг 4: Применяем weights к значениям\n",
    "    output = attention_weights @ V  # [seq_len, d_model]\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Тестируем на простом примере\n",
    "# Создаем случайные эмбеддинги (в реальности это будут word embeddings)\n",
    "seq_len = 5  # 5 слов в предложении\n",
    "d_model = 64  # размерность эмбеддингов\n",
    "d_k = 8  # размерность для Q, K\n",
    "\n",
    "X = np.random.randn(seq_len, d_model)  # наши \"эмбеддинги\"\n",
    "\n",
    "output, attention_weights = simple_self_attention(X, d_k)\n",
    "\n",
    "# Визуализируем attention матрицу\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_weights, cmap='Blues')\n",
    "plt.title(\"Self-Attention Weights (ручная реализация)\")\n",
    "plt.xlabel(\"Key tokens\")\n",
    "plt.ylabel(\"Query tokens\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c356e0dc119d87ca"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
