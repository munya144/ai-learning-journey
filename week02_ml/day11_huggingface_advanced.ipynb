{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T15:53:30.071505700Z",
     "start_time": "2026-02-03T15:40:45.318857200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Загружаем модель (самую быструю)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Модель загружена. Размер эмбеддингов: {model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# 2. Создаем эмбеддинги\n",
    "sentences = [\n",
    "    \"Собака играет в парке\",\n",
    "    \"Пёс бегает на улице\",  # Похожее по смыслу\n",
    "    \"Программист пишет код\",  # Совсем другое\n",
    "    \"Кот спит на диване\"  # Еще одно\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "print(f\"Создано {len(embeddings)} эмбеддингов\")\n",
    "\n",
    "# 3. Поиск похожих\n",
    "# Сравниваем первое предложение со всеми\n",
    "query_embedding = embeddings[0]\n",
    "similarities = cosine_similarity(\n",
    "    query_embedding.reshape(1, -1),\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "print(\"\\nПоиск похожих на 'Собака играет в парке':\")\n",
    "for i, sim in enumerate(similarities[0]):\n",
    "    print(f\"  '{sentences[i][:20]}...' → сходство: {sim:.3f}\")\n",
    "\n",
    "# 4. Тестируем другие модели БЫСТРО\n",
    "models_to_try = {\n",
    "    'fast': 'all-MiniLM-L6-v2',  # Быстрая и маленькая\n",
    "    'quality': 'all-mpnet-base-v2',  # Качественная\n",
    "    'qa': 'multi-qa-MiniLM-L6-cos-v1'  # Для вопросов-ответов\n",
    "}\n",
    "\n",
    "# Тест скорости и качества\n",
    "test_sentence = \"Как работает искусственный интеллект?\"\n",
    "\n",
    "for name, model_name in models_to_try.items():\n",
    "    try:\n",
    "        test_model = SentenceTransformer(model_name)\n",
    "        emb = test_model.encode(test_sentence)\n",
    "        print(f\"\\n{name} модель: {model_name}\")\n",
    "        print(f\"  Размер эмбеддинга: {emb.shape}\")\n",
    "        del test_model\n",
    "    except:\n",
    "        print(f\"\\nНе удалось загрузить {model_name}\")"
   ],
   "id": "c62cace6b36b35ae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb2c1998a9d74c3593e8c471a8affb27"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\6muni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\6muni\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a148d6a629954bb28255a6d9f1190174"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2299ae89af33452d909a11c4b1149c1d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd97942060834b8586396a252da3d937"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "274cc3b8201d45fe93f69c58f3a4dca0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eef51df03f4549b39461af2839d10ea6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b6cd1dbcc3540e3aa765ac4e7c0ac0b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2be872ac9064a809a8d773ccd7e4b18"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55524cf8b45344d9bae39aaf3cd617c3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9072f73e9314438a96218ef6ed3e2a02"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b175a63c76c46348b407a448e287ec6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fbbdc9357da4ddf9af80a31b33b16c5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загружена. Размер эмбеддингов: 384\n",
      "Создано 4 эмбеддингов\n",
      "\n",
      "Поиск похожих на 'Собака играет в парке':\n",
      "  'Собака играет в парк...' → сходство: 1.000\n",
      "  'Пёс бегает на улице...' → сходство: 0.666\n",
      "  'Программист пишет ко...' → сходство: 0.574\n",
      "  'Кот спит на диване...' → сходство: 0.622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54b581d24f8b4dc18e85b66ab49eabc6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fast модель: all-MiniLM-L6-v2\n",
      "  Размер эмбеддинга: (384,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ba34dd8c3224c40b31a753804cfc547"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\6muni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\6muni\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01c19931b63d4378baf677f7f635b79c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ca7e0601aa0478a9785d446d2cfdb3c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd373ac0a171455f9f03590b353bcf34"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "509e8dc30916431d9091a1d2d12de788"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "797176967ea0460ba0fad3229687a43a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b769c132181e4c2d8f177ef92499d419"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MPNetModel LOAD REPORT from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12496e7b6648474fa326d7baca93795d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0abe734f6de04364ab685863830c3b7f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5314c2d50ea442d9a5c04b255fe4e292"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "872c72f6e7cb4b7289faae0735943da7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c05687456ac34652bc8cd341b55362d5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "quality модель: all-mpnet-base-v2\n",
      "  Размер эмбеддинга: (768,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "adeffd73bd3d4477956b0c894bc08e39"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\6muni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\6muni\\.cache\\huggingface\\hub\\models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0815ccbe01ba4e52a7a63343d9799556"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "caa02597f54143e29b9f49829db49828"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2e76cb4618943e38a17c02dec77354a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f29ab463e8d464c8ced6d53435ee764"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17d8b74afd8e42e3a1a5d6ad1c3add17"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46ae7f9d2718415a8fd77b282dde1afa"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9e9deb073964e3ab2e237702cda9496"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a6c2042da0c405b82db46f34163b928"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77cdfbde7c6744ed8cdc489614765405"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "658df46813334e4280acb70e76f3e0cf"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc44fc2ec6c64102adac2fff82b4683f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "qa модель: multi-qa-MiniLM-L6-cos-v1\n",
      "  Размер эмбеддинга: (384,)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T16:08:50.283782800Z",
     "start_time": "2026-02-03T16:08:44.785733300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# 1. Берем маленький датасет (только 500 примеров)\n",
    "dataset = load_dataset(\"imdb\", split='train[:500]')  # Только первые 500\n",
    "\n",
    "# 2. Загружаем модель и токенизатор\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "# 3. Токенизация (быстро и просто)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 4. Разделяем на train/test (80/20)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# 5. Настраиваем обучение (ОЧЕНЬ БЫСТРО)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_model\",\n",
    "    num_train_epochs=1,  # ТОЛЬКО 1 эпоха!\n",
    "    per_device_train_batch_size=4,  # Маленький батч\n",
    "    save_strategy=\"no\",  # Не сохраняем\n",
    "    report_to=\"none\",  # Не отправляем отчеты\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 6. Обучаем (5-10 минут)\n",
    "print(\"Начинаем обучение...\")\n",
    "trainer.train()\n",
    "print(\"Обучение завершено!\")\n",
    "\n",
    "# 7. Быстрая проверка\n",
    "test_text = \"This movie was terrible, I hated it!\"\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "sentiment = \"NEGATIVE\" if predictions[0][0] > predictions[0][1] else \"POSITIVE\"\n",
    "print(f\"\\nТест: '{test_text}'\")\n",
    "print(f\"Предсказание: {sentiment} ({predictions[0]})\")"
   ],
   "id": "785efee4944d75dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78bba37eec824bfaa738412ed1db9f0d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8aabf03c234d4dababc2af776a823886"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a variable of type <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtokenize_function\u001B[39m(examples):\n\u001B[32m     22\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m tokenized_dataset = \u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenize_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[38;5;66;03m# 4. Разделяем на train/test (80/20)\u001B[39;00m\n\u001B[32m     28\u001B[39m split_dataset = tokenized_dataset.train_test_split(test_size=\u001B[32m0.2\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001B[39m, in \u001B[36mtransmit_format.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    555\u001B[39m self_format = {\n\u001B[32m    556\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_type,\n\u001B[32m    557\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mformat_kwargs\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_kwargs,\n\u001B[32m    558\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcolumns\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_columns,\n\u001B[32m    559\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moutput_all_columns\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._output_all_columns,\n\u001B[32m    560\u001B[39m }\n\u001B[32m    561\u001B[39m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m562\u001B[39m out: Union[\u001B[33m\"\u001B[39m\u001B[33mDataset\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mDatasetDict\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    563\u001B[39m datasets: \u001B[38;5;28mlist\u001B[39m[\u001B[33m\"\u001B[39m\u001B[33mDataset\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mlist\u001B[39m(out.values()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[32m    564\u001B[39m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:3343\u001B[39m, in \u001B[36mDataset.map\u001B[39m\u001B[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001B[39m\n\u001B[32m   3341\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3342\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m unprocessed_kwargs \u001B[38;5;129;01min\u001B[39;00m unprocessed_kwargs_per_job:\n\u001B[32m-> \u001B[39m\u001B[32m3343\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mDataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43munprocessed_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3344\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcheck_if_shard_done\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3346\u001B[39m \u001B[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:3699\u001B[39m, in \u001B[36mDataset._map_single\u001B[39m\u001B[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001B[39m\n\u001B[32m   3697\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3698\u001B[39m     _time = time.time()\n\u001B[32m-> \u001B[39m\u001B[32m3699\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miter_outputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard_iterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3700\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_examples_in_batch\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3701\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mupdate_data\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:3649\u001B[39m, in \u001B[36mDataset._map_single.<locals>.iter_outputs\u001B[39m\u001B[34m(shard_iterable)\u001B[39m\n\u001B[32m   3647\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3648\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m shard_iterable:\n\u001B[32m-> \u001B[39m\u001B[32m3649\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m i, \u001B[43mapply_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffset\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:3573\u001B[39m, in \u001B[36mDataset._map_single.<locals>.apply_function\u001B[39m\u001B[34m(pa_inputs, indices, offset)\u001B[39m\n\u001B[32m   3571\u001B[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001B[32m   3572\u001B[39m processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m3573\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mprepare_outputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessed_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:3537\u001B[39m, in \u001B[36mDataset._map_single.<locals>.prepare_outputs\u001B[39m\u001B[34m(pa_inputs, inputs, processed_inputs)\u001B[39m\n\u001B[32m   3535\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3536\u001B[39m     returned_lazy_dict = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3537\u001B[39m \u001B[43mvalidate_function_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocessed_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3538\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m shard._format_type \u001B[38;5;129;01mor\u001B[39;00m input_columns:\n\u001B[32m   3539\u001B[39m     \u001B[38;5;66;03m# TODO(QL, MS): ideally the behavior should be the same even if the dataset is formatted (may require major release)\u001B[39;00m\n\u001B[32m   3540\u001B[39m     inputs_to_merge = \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(pa_inputs.column_names, pa_inputs.itercolumns()))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\datasets\\arrow_dataset.py:3477\u001B[39m, in \u001B[36mDataset._map_single.<locals>.validate_function_output\u001B[39m\u001B[34m(processed_inputs)\u001B[39m\n\u001B[32m   3475\u001B[39m     allowed_processed_inputs_types += (pl.DataFrame,)\n\u001B[32m   3476\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m processed_inputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, allowed_processed_inputs_types):\n\u001B[32m-> \u001B[39m\u001B[32m3477\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[32m   3478\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mProvided `function` which is applied to all elements of table returns a variable of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(processed_inputs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3479\u001B[39m     )\n\u001B[32m   3480\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m batched \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, Mapping):\n\u001B[32m   3481\u001B[39m     allowed_batch_return_types = (\u001B[38;5;28mlist\u001B[39m, np.ndarray, pd.Series)\n",
      "\u001B[31mTypeError\u001B[39m: Provided `function` which is applied to all elements of table returns a variable of type <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T16:09:08.382477500Z",
     "start_time": "2026-02-03T16:09:03.741817100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# 1. Загружаем модель для семантического поиска\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# 2. Наши \"документы\" (вместо PDF)\n",
    "documents = [\n",
    "    \"Искусственный интеллект — это модели, которые учатся на данных.\",\n",
    "    \"Машинное обучение позволяет компьютерам учиться без явного программирования.\",\n",
    "    \"Python популярный язык для анализа данных и машинного обучения.\",\n",
    "    \"Нейронные сети состоят из слоев нейронов, которые обрабатывают информацию.\",\n",
    "    \"Глубокое обучение использует многослойные нейронные сети.\"\n",
    "]\n",
    "\n",
    "# 3. Создаем эмбеддинги для документов\n",
    "doc_embeddings = model.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "# 4. Функция поиска\n",
    "def semantic_search(query, documents, embeddings, top_k=2):\n",
    "    # Эмбеддинг запроса\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Ищем похожие\n",
    "    cos_scores = util.cos_sim(query_embedding, embeddings)[0]\n",
    "\n",
    "    # Берем топ-K результатов\n",
    "    top_results = np.argsort(-cos_scores.cpu().numpy())[:top_k]\n",
    "\n",
    "    print(f\"\\nЗапрос: '{query}'\")\n",
    "    print(f\"Найдено {len(top_results)} результатов:\")\n",
    "\n",
    "    for idx in top_results:\n",
    "        print(f\"\\nРейтинг: {cos_scores[idx]:.3f}\")\n",
    "        print(f\"Документ: {documents[idx]}\")\n",
    "\n",
    "\n",
    "# 5. Тестируем поиск\n",
    "queries = [\n",
    "    \"Что такое машинное обучение?\",\n",
    "    \"Какой язык для ИИ?\",\n",
    "    \"Что такое нейросети?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    semantic_search(query, documents, doc_embeddings)\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# 6. Сохраняем эмбеддинги для будущего использования\n",
    "np.save(\"document_embeddings.npy\", doc_embeddings.cpu().numpy())\n",
    "with open(\"documents.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc in documents:\n",
    "        f.write(doc + \"\\n\")\n",
    "\n",
    "print(\"\\nЭмбеддинги сохранены в 'document_embeddings.npy'\")\n",
    "print(\"Тексты сохранены в 'documents.txt'\")"
   ],
   "id": "13851f766d4c6c59",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "452a5bb4e586462f9ee9c37ce102660b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Запрос: 'Что такое машинное обучение?'\n",
      "Найдено 2 результатов:\n",
      "\n",
      "Рейтинг: 0.607\n",
      "Документ: Машинное обучение позволяет компьютерам учиться без явного программирования.\n",
      "\n",
      "Рейтинг: 0.484\n",
      "Документ: Глубокое обучение использует многослойные нейронные сети.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Запрос: 'Какой язык для ИИ?'\n",
      "Найдено 2 результатов:\n",
      "\n",
      "Рейтинг: 0.638\n",
      "Документ: Искусственный интеллект — это модели, которые учатся на данных.\n",
      "\n",
      "Рейтинг: 0.528\n",
      "Документ: Python популярный язык для анализа данных и машинного обучения.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Запрос: 'Что такое нейросети?'\n",
      "Найдено 2 результатов:\n",
      "\n",
      "Рейтинг: 0.563\n",
      "Документ: Искусственный интеллект — это модели, которые учатся на данных.\n",
      "\n",
      "Рейтинг: 0.530\n",
      "Документ: Нейронные сети состоят из слоев нейронов, которые обрабатывают информацию.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Эмбеддинги сохранены в 'document_embeddings.npy'\n",
      "Тексты сохранены в 'documents.txt'\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T18:32:50.868367300Z",
     "start_time": "2026-02-03T18:32:42.991043600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# 1. Загружаем данные (МАЛЕНЬКУЮ часть)\n",
    "print(\"Загружаем данные...\")\n",
    "dataset = load_dataset(\"imdb\", split='train[:500]')  # Только 500 примеров\n",
    "\n",
    "# 2. Загружаем модель и токенизатор\n",
    "print(\"Загружаем модель...\")\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Устанавливаем pad_token если его нет\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token or '[PAD]'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Токенизация\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128,  # Уменьшаем для скорости\n",
    "        padding=\"max_length\"  # Явно указываем padding\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Токенизируем данные...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 4. Разделение данных\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} примеров\")\n",
    "print(f\"Eval: {len(eval_dataset)} примеров\")\n",
    "\n",
    "# 5. DataCollator (теперь он не нужен, т.к. мы уже сделали padding)\n",
    "# Но создадим для примера\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 6. ИСПРАВЛЕННЫЕ TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    # Обязательные параметры\n",
    "    output_dir=\"./results\",  # Куда сохранять\n",
    "\n",
    "    # Параметры обучения\n",
    "    num_train_epochs=1,  # Одна эпоха\n",
    "    per_device_train_batch_size=8,  # Размер батча\n",
    "    per_device_eval_batch_size=8,  # Для валидации\n",
    "\n",
    "    # Оптимизатор\n",
    "    learning_rate=2e-5,  # Скорость обучения\n",
    "    weight_decay=0.01,  # Регуляризация\n",
    "\n",
    "    # Логирование и сохранение\n",
    "    logging_strategy=\"steps\",  # Логировать по шагам\n",
    "    logging_steps=10,  # Каждые 10 шагов\n",
    "    save_strategy=\"no\",  # Не сохранять чекпоинты\n",
    "\n",
    "    # Валидация\n",
    "    # Не оценивать во время обучения\n",
    "\n",
    "    # Прочее\n",
    "    report_to=\"none\",  # Без внешних логов\n",
    "    remove_unused_columns=True,  # Удалить лишние колонки\n",
    "    push_to_hub=False,  # Не загружать в облако\n",
    ")\n",
    "\n",
    "# 7. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Можно убрать если evaluation_strategy=\"no\"\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 8. Обучение\n",
    "print(\"\\nНачинаем обучение...\")\n",
    "trainer.train()\n",
    "print(\"Обучение завершено!\")\n",
    "\n",
    "# 9. Тестирование\n",
    "test_text = \"This movie was terrible!\"\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(f\"\\nТест: '{test_text}'\")\n",
    "print(f\"Вероятности: NEG={predictions[0][0]:.3f}, POS={predictions[0][1]:.3f}\")"
   ],
   "id": "b76ede2aec28b690",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем данные...\n",
      "Загружаем модель...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "416b8f9a751e46c1ad7b0b7a0a605b72"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизируем данные...\n",
      "Train: 400 примеров\n",
      "Eval: 100 примеров\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=1.1.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=1.1.0'`",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 51\u001B[39m\n\u001B[32m     48\u001B[39m data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\u001B[32m     50\u001B[39m \u001B[38;5;66;03m# 6. ИСПРАВЛЕННЫЕ TrainingArguments\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m training_args = \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Обязательные параметры\u001B[39;49;00m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./results\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Куда сохранять\u001B[39;49;00m\n\u001B[32m     54\u001B[39m \n\u001B[32m     55\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Параметры обучения\u001B[39;49;00m\n\u001B[32m     56\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Одна эпоха\u001B[39;49;00m\n\u001B[32m     57\u001B[39m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Размер батча\u001B[39;49;00m\n\u001B[32m     58\u001B[39m \u001B[43m    \u001B[49m\u001B[43mper_device_eval_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Для валидации\u001B[39;49;00m\n\u001B[32m     59\u001B[39m \n\u001B[32m     60\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Оптимизатор\u001B[39;49;00m\n\u001B[32m     61\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2e-5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Скорость обучения\u001B[39;49;00m\n\u001B[32m     62\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Регуляризация\u001B[39;49;00m\n\u001B[32m     63\u001B[39m \n\u001B[32m     64\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Логирование и сохранение\u001B[39;49;00m\n\u001B[32m     65\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogging_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msteps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Логировать по шагам\u001B[39;49;00m\n\u001B[32m     66\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Каждые 10 шагов\u001B[39;49;00m\n\u001B[32m     67\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mno\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Не сохранять чекпоинты\u001B[39;49;00m\n\u001B[32m     68\u001B[39m \n\u001B[32m     69\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Валидация\u001B[39;49;00m\n\u001B[32m     70\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Не оценивать во время обучения\u001B[39;49;00m\n\u001B[32m     71\u001B[39m \n\u001B[32m     72\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Прочее\u001B[39;49;00m\n\u001B[32m     73\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreport_to\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mnone\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Без внешних логов\u001B[39;49;00m\n\u001B[32m     74\u001B[39m \u001B[43m    \u001B[49m\u001B[43mremove_unused_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Удалить лишние колонки\u001B[39;49;00m\n\u001B[32m     75\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpush_to_hub\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Не загружать в облако\u001B[39;49;00m\n\u001B[32m     76\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     78\u001B[39m \u001B[38;5;66;03m# 7. Trainer\u001B[39;00m\n\u001B[32m     79\u001B[39m trainer = Trainer(\n\u001B[32m     80\u001B[39m     model=model,\n\u001B[32m     81\u001B[39m     args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m     85\u001B[39m     data_collator=data_collator,\n\u001B[32m     86\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:112\u001B[39m, in \u001B[36m__init__\u001B[39m\u001B[34m(self, output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, enable_jit_checkpoint, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, use_cpu, seed, data_seed, bf16, fp16, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_config, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_for_metrics, eval_do_concat_batches, auto_find_batch_size, full_determinism, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices, use_cache)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1602\u001B[39m, in \u001B[36mTrainingArguments.__post_init__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1600\u001B[39m \u001B[38;5;66;03m# Initialize device before we proceed\u001B[39;00m\n\u001B[32m   1601\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available():\n\u001B[32m-> \u001B[39m\u001B[32m1602\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\n\u001B[32m   1604\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.torch_compile:\n\u001B[32m   1605\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_torch_tf32_available():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1857\u001B[39m, in \u001B[36mTrainingArguments.device\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1853\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1854\u001B[39m \u001B[33;03mThe device used by this process.\u001B[39;00m\n\u001B[32m   1855\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1856\u001B[39m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[33m\"\u001B[39m\u001B[33mtorch\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m-> \u001B[39m\u001B[32m1857\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_setup_devices\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\functools.py:1026\u001B[39m, in \u001B[36mcached_property.__get__\u001B[39m\u001B[34m(self, instance, owner)\u001B[39m\n\u001B[32m   1024\u001B[39m val = cache.get(\u001B[38;5;28mself\u001B[39m.attrname, _NOT_FOUND)\n\u001B[32m   1025\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m val \u001B[38;5;129;01mis\u001B[39;00m _NOT_FOUND:\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m     val = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1027\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1028\u001B[39m         cache[\u001B[38;5;28mself\u001B[39m.attrname] = val\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1744\u001B[39m, in \u001B[36mTrainingArguments._setup_devices\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[32m   1743\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[32m-> \u001B[39m\u001B[32m1744\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m   1745\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m`: \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1746\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPlease run `pip install transformers[torch]` or `pip install \u001B[39m\u001B[33m'\u001B[39m\u001B[33maccelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1747\u001B[39m         )\n\u001B[32m   1748\u001B[39m \u001B[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001B[39;00m\n\u001B[32m   1749\u001B[39m accelerator_state_kwargs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = {\u001B[33m\"\u001B[39m\u001B[33menabled\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33muse_configured_state\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}\n",
      "\u001B[31mImportError\u001B[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=1.1.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=1.1.0'`"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T05:06:01.654936300Z",
     "start_time": "2026-02-04T05:05:27.342584300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Импорт необходимых библиотек\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ПРОЕКТ: SEMANTIC SEARCH (ПОИСК ПО СМЫСЛУ)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================\n",
    "# 1. ПОДГОТОВКА ДАННЫХ (вместо PDF)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n1. Подготавливаем данные...\")\n",
    "\n",
    "# Вместо PDF создадим простые тексты о технологиях\n",
    "documents = [\n",
    "    # Документ 1: Об ИИ\n",
    "    \"Искусственный интеллект (ИИ) — это способность компьютеров решать задачи, которые обычно требуют человеческого интеллекта.\",\n",
    "\n",
    "    # Документ 2: О машинном обучении\n",
    "    \"Машинное обучение — это часть ИИ, где компьютеры учатся на данных без явного программирования.\",\n",
    "\n",
    "    # Документ 3: О нейросетях\n",
    "    \"Нейронные сети — это алгоритмы, которые имитируют работу человеческого мозга для распознавания образов.\",\n",
    "\n",
    "    # Документ 4: О Python\n",
    "    \"Python — популярный язык программирования для анализа данных и машинного обучения.\",\n",
    "\n",
    "    # Документ 5: О больших данных\n",
    "    \"Большие данные (Big Data) — это огромные объемы информации, которые трудно обрабатывать традиционными методами.\",\n",
    "\n",
    "    # Документ 6: Об облачных технологиях\n",
    "    \"Облачные вычисления позволяют хранить и обрабатывать данные на удаленных серверах через интернет.\",\n",
    "\n",
    "    # Документ 7: О кибербезопасности\n",
    "    \"Кибербезопасность защищает компьютеры, сети и данные от цифровых атак.\",\n",
    "\n",
    "    # Документ 8: О блокчейне\n",
    "    \"Блокчейн — это технология хранения данных в виде цепочки блоков, которая обеспечивает безопасность и прозрачность.\",\n",
    "\n",
    "    # Документ 9: О интернете вещей\n",
    "    \"Интернет вещей (IoT) соединяет обычные предметы с интернетом для сбора и обмена данными.\",\n",
    "\n",
    "    # Документ 10: О виртуальной реальности\n",
    "    \"Виртуальная реальность создает искусственную среду, с которой пользователь может взаимодействовать.\"\n",
    "]\n",
    "\n",
    "print(f\"   Создано {len(documents)} документов\")\n",
    "print(\"\\n   Пример документа 1:\")\n",
    "print(f\"   '{documents[0][:50]}...'\")\n",
    "\n",
    "# ============================================\n",
    "# 2. ЗАГРУЗКА МОДЕЛИ ДЛЯ ЭМБЕДДИНГОВ\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n2. Загружаем модель для создания эмбеддингов...\")\n",
    "\n",
    "# Используем быструю модель\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"   Модель: all-MiniLM-L6-v2\")\n",
    "print(f\"   Размер эмбеддинга: {model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# ============================================\n",
    "# 3. СОЗДАНИЕ ЭМБЕДДИНГОВ\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n3. Создаем эмбеддинги для документов...\")\n",
    "\n",
    "# Преобразуем тексты в векторы (эмбеддинги)\n",
    "document_embeddings = model.encode(\n",
    "    documents,\n",
    "    convert_to_tensor=False,  # Используем numpy для простоты\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True  # Нормализуем для более точного сравнения\n",
    ")\n",
    "\n",
    "print(f\"   Создано {len(document_embeddings)} эмбеддингов\")\n",
    "print(f\"   Размер каждого эмбеддинга: {document_embeddings[0].shape}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. СОХРАНЕНИЕ ЭМБЕДДИНГОВ\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n4. Сохраняем эмбеддинги и документы...\")\n",
    "\n",
    "# Сохраняем эмбеддинги в файл\n",
    "np.save('document_embeddings.npy', document_embeddings)\n",
    "\n",
    "# Сохраняем тексты документов\n",
    "with open('documents.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, doc in enumerate(documents):\n",
    "        f.write(f\"ДОКУМЕНТ {i + 1}:\\n\")\n",
    "        f.write(doc + \"\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"   ✅ Эмбеддинги сохранены в 'document_embeddings.npy'\")\n",
    "print(\"   ✅ Тексты сохранены в 'documents.txt'\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. ФУНКЦИЯ ПОИСКА\n",
    "# ============================================\n",
    "\n",
    "def semantic_search(query, documents, embeddings, top_k=3):\n",
    "    \"\"\"\n",
    "    Ищет документы, наиболее похожие на запрос по смыслу\n",
    "\n",
    "    Параметры:\n",
    "    - query: строка с поисковым запросом\n",
    "    - documents: список документов\n",
    "    - embeddings: эмбеддинги документов\n",
    "    - top_k: сколько результатов показать\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nПоиск: '{query}'\")\n",
    "    print(f\"   Ищем {top_k} наиболее релевантных документа...\")\n",
    "\n",
    "    # 1. Создаем эмбеддинг для запроса\n",
    "    query_embedding = model.encode(\n",
    "        query,\n",
    "        convert_to_tensor=False,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # 2. Считаем косинусную схожесть с каждым документом\n",
    "    similarities = cosine_similarity(\n",
    "        [query_embedding],  # Запрос (в виде матрицы 1x384)\n",
    "        embeddings  # Все документы\n",
    "    )\n",
    "\n",
    "    # 3. Берем индексы top_k наиболее похожих документов\n",
    "    top_indices = np.argsort(similarities[0])[-top_k:][::-1]\n",
    "\n",
    "    # 4. Выводим результаты\n",
    "    print(f\"\\n    РЕЗУЛЬТАТЫ ПОИСКА:\")\n",
    "    print(\"   \" + \"=\" * 50)\n",
    "\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        similarity_score = similarities[0][idx]\n",
    "\n",
    "        print(f\"\\n   #{i + 1} (сходство: {similarity_score:.3f}):\")\n",
    "        print(f\"   Документ {idx + 1}:\")\n",
    "\n",
    "        # Показываем начало документа\n",
    "        doc_preview = documents[idx]\n",
    "        if len(doc_preview) > 80:\n",
    "            doc_preview = doc_preview[:80] + \"...\"\n",
    "\n",
    "        print(f\"   '{doc_preview}'\")\n",
    "\n",
    "    return top_indices, similarities[0][top_indices]\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. ТЕСТИРОВАНИЕ ПОИСКА\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. ТЕСТИРУЕМ ПОИСК ПО СМЫСЛУ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Тестовые запросы\n",
    "test_queries = [\n",
    "    \"компьютеры и интеллект\",  # Должен найти документ 1 (ИИ)\n",
    "    \"обучение на данных\",  # Должен найти документ 2 (ML)\n",
    "    \"язык программирования\",  # Должен найти документ 4 (Python)\n",
    "    \"безопасность в интернете\",  # Должен найти документ 7 (кибербезопасность)\n",
    "    \"что-то совсем не связанное\"  # Не должен найти ничего релевантного\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    semantic_search(query, documents, document_embeddings, top_k=2)\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "# ============================================\n",
    "# 7. ДОПОЛНИТЕЛЬНЫЕ ВОЗМОЖНОСТИ\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ДОПОЛНИТЕЛЬНЫЕ ФУНКЦИИ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Функция для поиска похожих документов\n",
    "def find_similar_documents(doc_index, documents, embeddings, top_k=3):\n",
    "    \"\"\"Находит документы, похожие на заданный документ\"\"\"\n",
    "\n",
    "    print(f\"\\nПохожие на документ {doc_index + 1}:\")\n",
    "    print(f\"   '{documents[doc_index][:50]}...'\")\n",
    "\n",
    "    # Считаем схожесть с другими документами\n",
    "    similarities = cosine_similarity(\n",
    "        [embeddings[doc_index]],\n",
    "        embeddings\n",
    "    )\n",
    "\n",
    "    # Исключаем сам документ\n",
    "    all_indices = np.argsort(similarities[0])[::-1]\n",
    "    similar_indices = [idx for idx in all_indices if idx != doc_index][:top_k]\n",
    "\n",
    "    print(f\"\\n   Самые похожие документы:\")\n",
    "    for i, idx in enumerate(similar_indices):\n",
    "        score = similarities[0][idx]\n",
    "        print(f\"   {i + 1}. Док {idx + 1} (сходство: {score:.3f}): {documents[idx][:60]}...\")\n",
    "\n",
    "\n",
    "# Тестируем\n",
    "print(\"\\n1. Поиск похожих документов:\")\n",
    "find_similar_documents(0, documents, document_embeddings)  # Документ про ИИ\n",
    "find_similar_documents(3, documents, document_embeddings)  # Документ про Python\n",
    "\n",
    "\n",
    "# Функция для кластеризации документов\n",
    "def simple_clustering(documents, embeddings, n_clusters=3):\n",
    "    \"\"\"Простая кластеризация документов\"\"\"\n",
    "\n",
    "    print(f\"\\n2. Кластеризация документов на {n_clusters} группы:\")\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    # Выполняем K-means кластеризацию\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # Выводим результаты\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_docs = np.where(clusters == cluster_id)[0]\n",
    "\n",
    "        print(f\"\\n   📂 Кластер {cluster_id + 1} ({len(cluster_docs)} документов):\")\n",
    "        for doc_idx in cluster_docs[:3]:  # Показываем первые 3 документа\n",
    "            print(f\"   - Док {doc_idx + 1}: {documents[doc_idx][:50]}...\")\n",
    "        if len(cluster_docs) > 3:\n",
    "            print(f\"   - ... и еще {len(cluster_docs) - 3} документов\")\n",
    "\n",
    "\n",
    "# Тестируем кластеризацию\n",
    "simple_clustering(documents, document_embeddings, n_clusters=3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ ПРОЕКТ ЗАВЕРШЕН!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nВы можете использовать сохраненные файлы:\")\n",
    "print(\"1. document_embeddings.npy - векторы документов\")\n",
    "print(\"2. documents.txt - тексты документов\")\n",
    "print(\"\\nДля загрузки используйте:\")\n",
    "print(\"embeddings = np.load('document_embeddings.npy')\")"
   ],
   "id": "ba825d6ec8aa93c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ПРОЕКТ: SEMANTIC SEARCH (ПОИСК ПО СМЫСЛУ)\n",
      "============================================================\n",
      "\n",
      "1. Подготавливаем данные...\n",
      "   Создано 10 документов\n",
      "\n",
      "   Пример документа 1:\n",
      "   'Искусственный интеллект (ИИ) — это способность ком...'\n",
      "\n",
      "2. Загружаем модель для создания эмбеддингов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6c3e40180254734945ac8840ac57305"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Модель: all-MiniLM-L6-v2\n",
      "   Размер эмбеддинга: 384\n",
      "\n",
      "3. Создаем эмбеддинги для документов...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f070f0802f0b4e78b0535fb9356ed7f4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Создано 10 эмбеддингов\n",
      "   Размер каждого эмбеддинга: (384,)\n",
      "\n",
      "4. Сохраняем эмбеддинги и документы...\n",
      "   ✅ Эмбеддинги сохранены в 'document_embeddings.npy'\n",
      "   ✅ Тексты сохранены в 'documents.txt'\n",
      "\n",
      "============================================================\n",
      "6. ТЕСТИРУЕМ ПОИСК ПО СМЫСЛУ\n",
      "============================================================\n",
      "\n",
      "Поиск: 'компьютеры и интеллект'\n",
      "   Ищем 2 наиболее релевантных документа...\n",
      "\n",
      "    РЕЗУЛЬТАТЫ ПОИСКА:\n",
      "   ==================================================\n",
      "\n",
      "   #1 (сходство: 0.737):\n",
      "   Документ 7:\n",
      "   'Кибербезопасность защищает компьютеры, сети и данные от цифровых атак.'\n",
      "\n",
      "   #2 (сходство: 0.713):\n",
      "   Документ 1:\n",
      "   'Искусственный интеллект (ИИ) — это способность компьютеров решать задачи, которы...'\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Поиск: 'обучение на данных'\n",
      "   Ищем 2 наиболее релевантных документа...\n",
      "\n",
      "    РЕЗУЛЬТАТЫ ПОИСКА:\n",
      "   ==================================================\n",
      "\n",
      "   #1 (сходство: 0.789):\n",
      "   Документ 2:\n",
      "   'Машинное обучение — это часть ИИ, где компьютеры учатся на данных без явного про...'\n",
      "\n",
      "   #2 (сходство: 0.731):\n",
      "   Документ 6:\n",
      "   'Облачные вычисления позволяют хранить и обрабатывать данные на удаленных сервера...'\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Поиск: 'язык программирования'\n",
      "   Ищем 2 наиболее релевантных документа...\n",
      "\n",
      "    РЕЗУЛЬТАТЫ ПОИСКА:\n",
      "   ==================================================\n",
      "\n",
      "   #1 (сходство: 0.678):\n",
      "   Документ 2:\n",
      "   'Машинное обучение — это часть ИИ, где компьютеры учатся на данных без явного про...'\n",
      "\n",
      "   #2 (сходство: 0.659):\n",
      "   Документ 3:\n",
      "   'Нейронные сети — это алгоритмы, которые имитируют работу человеческого мозга для...'\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Поиск: 'безопасность в интернете'\n",
      "   Ищем 2 наиболее релевантных документа...\n",
      "\n",
      "    РЕЗУЛЬТАТЫ ПОИСКА:\n",
      "   ==================================================\n",
      "\n",
      "   #1 (сходство: 0.678):\n",
      "   Документ 7:\n",
      "   'Кибербезопасность защищает компьютеры, сети и данные от цифровых атак.'\n",
      "\n",
      "   #2 (сходство: 0.667):\n",
      "   Документ 2:\n",
      "   'Машинное обучение — это часть ИИ, где компьютеры учатся на данных без явного про...'\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Поиск: 'что-то совсем не связанное'\n",
      "   Ищем 2 наиболее релевантных документа...\n",
      "\n",
      "    РЕЗУЛЬТАТЫ ПОИСКА:\n",
      "   ==================================================\n",
      "\n",
      "   #1 (сходство: 0.737):\n",
      "   Документ 3:\n",
      "   'Нейронные сети — это алгоритмы, которые имитируют работу человеческого мозга для...'\n",
      "\n",
      "   #2 (сходство: 0.661):\n",
      "   Документ 2:\n",
      "   'Машинное обучение — это часть ИИ, где компьютеры учатся на данных без явного про...'\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ДОПОЛНИТЕЛЬНЫЕ ФУНКЦИИ\n",
      "============================================================\n",
      "\n",
      "1. Поиск похожих документов:\n",
      "\n",
      "Похожие на документ 1:\n",
      "   'Искусственный интеллект (ИИ) — это способность ком...'\n",
      "\n",
      "   Самые похожие документы:\n",
      "   1. Док 2 (сходство: 0.704): Машинное обучение — это часть ИИ, где компьютеры учатся на д...\n",
      "   2. Док 7 (сходство: 0.698): Кибербезопасность защищает компьютеры, сети и данные от цифр...\n",
      "   3. Док 3 (сходство: 0.674): Нейронные сети — это алгоритмы, которые имитируют работу чел...\n",
      "\n",
      "Похожие на документ 4:\n",
      "   'Python — популярный язык программирования для анал...'\n",
      "\n",
      "   Самые похожие документы:\n",
      "   1. Док 2 (сходство: 0.558): Машинное обучение — это часть ИИ, где компьютеры учатся на д...\n",
      "   2. Док 8 (сходство: 0.529): Блокчейн — это технология хранения данных в виде цепочки бло...\n",
      "   3. Док 6 (сходство: 0.507): Облачные вычисления позволяют хранить и обрабатывать данные ...\n",
      "\n",
      "2. Кластеризация документов на 3 группы:\n",
      "\n",
      "   📂 Кластер 1 (8 документов):\n",
      "   - Док 1: Искусственный интеллект (ИИ) — это способность ком...\n",
      "   - Док 2: Машинное обучение — это часть ИИ, где компьютеры у...\n",
      "   - Док 3: Нейронные сети — это алгоритмы, которые имитируют ...\n",
      "   - ... и еще 5 документов\n",
      "\n",
      "   📂 Кластер 2 (1 документов):\n",
      "   - Док 4: Python — популярный язык программирования для анал...\n",
      "\n",
      "   📂 Кластер 3 (1 документов):\n",
      "   - Док 5: Большие данные (Big Data) — это огромные объемы ин...\n",
      "\n",
      "============================================================\n",
      "✅ ПРОЕКТ ЗАВЕРШЕН!\n",
      "============================================================\n",
      "\n",
      "Вы можете использовать сохраненные файлы:\n",
      "1. document_embeddings.npy - векторы документов\n",
      "2. documents.txt - тексты документов\n",
      "\n",
      "Для загрузки используйте:\n",
      "embeddings = np.load('document_embeddings.npy')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8fc340efa4ba0247"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
