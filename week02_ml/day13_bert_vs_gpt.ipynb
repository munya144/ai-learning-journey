{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:07:43.993268300Z",
     "start_time": "2026-02-04T15:07:41.974498200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 1. Создаем маленький датасет\n",
    "data = {\n",
    "    'text': [\n",
    "        'Фильм был отличный, очень понравился!',\n",
    "        'Ужасный опыт, не рекомендую.',\n",
    "        'Нормально, но могло быть лучше.',\n",
    "        'Лучший сервис, буду обращаться еще!',\n",
    "        'Разочарован, ожидал большего.'\n",
    "    ],\n",
    "    'label': [1, 0, 0, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 2. Загружаем модель\n",
    "model_name = \"blanchefort/rubert-base-cased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# 3. Токенизируем\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 4. Параметры тренировки\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10,\n",
    "    logging_steps=5\n",
    ")\n",
    "\n",
    "# 5. Тренер и тренировка\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 6. Проверка\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Модель на правильное устройство\n",
    "\n",
    "test_texts = [\n",
    "    \"Очень доволен покупкой!\",\n",
    "    \"Качество ужасное, деньги на ветер.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "\n",
    "    # Все данные на то же устройство, что и модель\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "    sentiment = \"ПОЗИТИВ\" if prediction == 1 else \"НЕГАТИВ\"\n",
    "    print(f\"Текст: {text}\")\n",
    "    print(f\"Результат: {sentiment}\\n\")"
   ],
   "id": "a2bf5cb7c7933671",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59832032bdaf4b10b4f29afcd75495e1"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification LOAD REPORT from: blanchefort/rubert-base-cased-sentiment\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92a14bd04275402985a965c07ee7f640"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=1.1.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=1.1.0'`",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 36\u001B[39m\n\u001B[32m     33\u001B[39m tokenized_datasets = dataset.map(tokenize_function, batched=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# 4. Параметры тренировки\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m training_args = \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./results\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\n\u001B[32m     42\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     44\u001B[39m \u001B[38;5;66;03m# 5. Тренер и тренировка\u001B[39;00m\n\u001B[32m     45\u001B[39m trainer = Trainer(\n\u001B[32m     46\u001B[39m     model=model,\n\u001B[32m     47\u001B[39m     args=training_args,\n\u001B[32m     48\u001B[39m     train_dataset=tokenized_datasets,\n\u001B[32m     49\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:112\u001B[39m, in \u001B[36m__init__\u001B[39m\u001B[34m(self, output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, enable_jit_checkpoint, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, use_cpu, seed, data_seed, bf16, fp16, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_config, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_for_metrics, eval_do_concat_batches, auto_find_batch_size, full_determinism, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices, use_cache)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1602\u001B[39m, in \u001B[36mTrainingArguments.__post_init__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1600\u001B[39m \u001B[38;5;66;03m# Initialize device before we proceed\u001B[39;00m\n\u001B[32m   1601\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available():\n\u001B[32m-> \u001B[39m\u001B[32m1602\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\n\u001B[32m   1604\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.torch_compile:\n\u001B[32m   1605\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_torch_tf32_available():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1857\u001B[39m, in \u001B[36mTrainingArguments.device\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1853\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1854\u001B[39m \u001B[33;03mThe device used by this process.\u001B[39;00m\n\u001B[32m   1855\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1856\u001B[39m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[33m\"\u001B[39m\u001B[33mtorch\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m-> \u001B[39m\u001B[32m1857\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_setup_devices\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\functools.py:1026\u001B[39m, in \u001B[36mcached_property.__get__\u001B[39m\u001B[34m(self, instance, owner)\u001B[39m\n\u001B[32m   1024\u001B[39m val = cache.get(\u001B[38;5;28mself\u001B[39m.attrname, _NOT_FOUND)\n\u001B[32m   1025\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m val \u001B[38;5;129;01mis\u001B[39;00m _NOT_FOUND:\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m     val = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1027\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1028\u001B[39m         cache[\u001B[38;5;28mself\u001B[39m.attrname] = val\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1744\u001B[39m, in \u001B[36mTrainingArguments._setup_devices\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[32m   1743\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[32m-> \u001B[39m\u001B[32m1744\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m   1745\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m`: \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1746\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPlease run `pip install transformers[torch]` or `pip install \u001B[39m\u001B[33m'\u001B[39m\u001B[33maccelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1747\u001B[39m         )\n\u001B[32m   1748\u001B[39m \u001B[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001B[39;00m\n\u001B[32m   1749\u001B[39m accelerator_state_kwargs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = {\u001B[33m\"\u001B[39m\u001B[33menabled\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33muse_configured_state\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}\n",
      "\u001B[31mImportError\u001B[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=1.1.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=1.1.0'`"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Объяснение: GPT - это декодерная модель. Генерирует текст последовательно, слово за словом.\n",
    "# Хороша для генерации текста: чат-боты, продолжение текста, творчество.\n",
    "\n",
    "# 1. Загружаем модель и токенизатор\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Устанавливаем padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# 2. Функция для генерации текста с разными параметрами\n",
    "def generate_text(prompt, temperature=1.0, top_k=50, top_p=0.9):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=50,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# 3. Промпт для генерации\n",
    "prompt = \"Сегодня был прекрасный день\"\n",
    "\n",
    "print(\"=== Генерация текста с разными параметрами ===\\n\")\n",
    "\n",
    "# Разные temperature\n",
    "print(\"1. Разная temperature (влияет на случайность):\")\n",
    "print(f\"   temperature=0.7 (более предсказуемо):\")\n",
    "print(f\"   {generate_text(prompt, temperature=0.7, top_k=50, top_p=0.9)[:100]}...\")\n",
    "print(f\"\\n   temperature=1.0 (стандарт):\")\n",
    "print(f\"   {generate_text(prompt, temperature=1.0, top_k=50, top_p=0.9)[:100]}...\")\n",
    "print(f\"\\n   temperature=1.5 (более креативно):\")\n",
    "print(f\"   {generate_text(prompt, temperature=1.5, top_k=50, top_p=0.9)[:100]}...\")\n",
    "\n",
    "# Разные top_k\n",
    "print(\"\\n\\n2. Разный top_k (сколько вариантов рассматривать):\")\n",
    "print(f\"   top_k=50 (только 50 лучших вариантов):\")\n",
    "print(f\"   {generate_text(prompt, temperature=1.0, top_k=50, top_p=0.9)[:100]}...\")\n",
    "print(f\"\\n   top_k=100 (100 лучших вариантов):\")\n",
    "print(f\"   {generate_text(prompt, temperature=1.0, top_k=100, top_p=0.9)[:100]}...\")\n",
    "\n",
    "# Разные top_p\n",
    "print(\"\\n\\n3. Разный top_p (nucleus sampling):\")\n",
    "print(f\"   top_p=0.9 (рассматриваем варианты с суммарной вероятностью 0.9):\")\n",
    "print(f\"   {generate_text(prompt, temperature=1.0, top_k=50, top_p=0.9)[:100]}...\")\n",
    "print(f\"\\n   top_p=0.95 (рассматриваем больше вариантов):\")\n",
    "print(f\"   {generate_text(prompt, temperature=1.0, top_k=50, top_p=0.95)[:100]}...\")\n",
    "\n",
    "# Объяснение параметров:\n",
    "# - temperature: чем выше, тем более случайный текст\n",
    "# - top_k: ограничивает выбор только K лучшими вариантами\n",
    "# - top_p: выбирает из минимального набора вариантов, чья сумма вероятностей ≥ p"
   ],
   "id": "97b098e3f454a53d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
